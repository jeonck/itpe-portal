{
  "id": "ensemble-001",
  "title": "앙상블 (Bagging, Boosting, Random Forest)",
  "category": "technical-focus",
  "subcategory": "알고리즘",
  "subjectCategories": [
    "AL",
    "DS"
  ],
  "difficulty": "advanced",
  "certifications": [
    "information-management"
  ],
  "keywords": [
    "Ensemble",
    "Bagging",
    "Boosting",
    "Random Forest",
    "XGBoost",
    "Stacking"
  ],
  "definition": "여러 약한 학습기를 결합하여 강력한 예측 모델을 만드는 기법 기법.",
  "operatingPrinciple": "- Bagging 동작: 1) Bootstrap Sampling으로 여러 학습 데이터셋 생성 (중복 허용 무작위 추출) → 2) 각 데이터셋으로 독립적인 모델 학습 → 3) 예측 시 모든 모델의 결과를 평균(회귀) 또는 다수결 투표(분류)로 결합\n- Random Forest 동작: 1) Bootstrap으로 데이터 샘플링 → 2) 각 노드 분할 시 전체 특징 중 일부만 무작위 선택 → 3) 선택된 특징에서 최적 분할 수행 → 4) 여러 트리 생성 후 결과 집계\n- Boosting 동작: 1) 초기 모델로 예측 → 2) 오분류 샘플의 가중치 증가 → 3) 가중치가 높은 샘플에 집중하여 다음 모델 학습 → 4) 순차적으로 모델 추가하며 오류 수정 → 5) 가중 평균으로 최종 예측\n- Gradient Boosting 동작: 1) 초기 예측값 설정 → 2) 실제값과 예측값의 잔차(residual) 계산 → 3) 잔차를 타겟으로 새로운 모델 학습 → 4) 학습률을 곱해 예측값에 추가 → 5) 반복하여 잔차 최소화\n- Stacking 동작: 1) 1차 모델들(base learners)로 예측 → 2) 1차 모델들의 예측값을 새로운 특징으로 사용 → 3) Meta-learner가 이 예측값들을 학습하여 최종 예측",
  "characteristics": [
    "장점: 과적합 감소, 정확도 향상, 안정성",
    "Bagging (Bootstrap Aggregating): 병렬, 무작위 샘플링, 평균/투표",
    "Random Forest: Decision Tree + Bagging + Feature Sampling",
    "특징: 각 트리는 무작위 데이터 + 무작위 특징, 분산 감소",
    "Boosting: 순차적, 이전 오류에 집중, 편향 감소",
    "AdaBoost: 오분류 샘플 가중치 증가",
    "Gradient Boosting: 잔차(Residual) 학습, 경사하강법",
    "XGBoost: 빠른 Gradient Boosting, 정규화, 병렬 처리, Kaggle 우승",
    "LightGBM: Leaf-wise 성장, 빠름, 대규모 데이터",
    "CatBoost: 범주형 데이터 자동 처리",
    "Stacking: 다층 앙상블, Meta-learner"
  ],
  "relatedTopics": [
    "ml-learning-types-001",
    "gradient-descent-001"
  ],
  "importance": 5,
  "trends": [
    "LightGBM",
    "CatBoost"
  ],
  "tags": [
    "2025"
  ]
}