---
category: technical-focus
certifications:
- information-management
difficulty: advanced
id: ensemble-001
importance: 5
keywords:
- Ensemble
- Bagging
- Boosting
- Random Forest
- XGBoost
- Stacking
relatedTopics:
- ml-learning-types-001
- gradient-descent-001
subcategory: 알고리즘
subjectCategories:
- AL
- DS
tags:
- '2025'
title: 앙상블 (Bagging, Boosting, Random Forest)
trends:
- LightGBM
- CatBoost
---

# 정의
여러 약한 학습기를 결합하여 강력한 예측 모델을 만드는 기법입니다.

## 특징
- 장점: 과적합 감소, 정확도 향상, 안정성
- Bagging (Bootstrap Aggregating): 병렬, 무작위 샘플링, 평균/투표
- Random Forest: Decision Tree + Bagging + Feature Sampling
- 특징: 각 트리는 무작위 데이터 + 무작위 특징, 분산 감소
- Boosting: 순차적, 이전 오류에 집중, 편향 감소
- AdaBoost: 오분류 샘플 가중치 증가
- Gradient Boosting: 잔차(Residual) 학습, 경사하강법
- XGBoost: 빠른 Gradient Boosting, 정규화, 병렬 처리, Kaggle 우승
- LightGBM: Leaf-wise 성장, 빠름, 대규모 데이터
- CatBoost: 범주형 데이터 자동 처리
- Stacking: 다층 앙상블, Meta-learner

## 최신 트렌드
- LightGBM
- CatBoost

# 정의
여러 약한 학습기를 결합하여 강력한 예측 모델을 만드는 기법입니다.

## 특징
- 장점: 과적합 감소, 정확도 향상, 안정성
- Bagging (Bootstrap Aggregating): 병렬, 무작위 샘플링, 평균/투표
- Random Forest: Decision Tree + Bagging + Feature Sampling
- 특징: 각 트리는 무작위 데이터 + 무작위 특징, 분산 감소
- Boosting: 순차적, 이전 오류에 집중, 편향 감소
- AdaBoost: 오분류 샘플 가중치 증가
- Gradient Boosting: 잔차(Residual) 학습, 경사하강법
- XGBoost: 빠른 Gradient Boosting, 정규화, 병렬 처리, Kaggle 우승
- LightGBM: Leaf-wise 성장, 빠름, 대규모 데이터
- CatBoost: 범주형 데이터 자동 처리
- Stacking: 다층 앙상블, Meta-learner

## 최신 트렌드
- LightGBM
- CatBoost