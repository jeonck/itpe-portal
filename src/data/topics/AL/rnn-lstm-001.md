---
category: technical-focus
certifications:
- information-management
difficulty: advanced
id: rnn-lstm-001
importance: 5
keywords:
- RNN
- LSTM
- GRU
- Sequence Model
- Time Series
- NLP
relatedTopics:
- transformer-001
- ai-deep-learning-001
subcategory: 알고리즘
subjectCategories:
- AL
- DS
tags:
- '2025'
title: RNN & LSTM (순환 신경망)
trends:
- Transformer 대체
- Bidirectional RNN
---

# 정의
시퀀스 데이터의 시간적 의존성을 학습하는 순환 신경망으로, 자연어와 시계열 처리에 사용됩니다.

## 특징
- RNN (Recurrent Neural Network): 이전 상태를 현재 입력에 반영, 순환 구조
- 문제점: Vanishing Gradient (기울기 소실), Long-term Dependency 학습 어려움
- LSTM (Long Short-Term Memory): Cell State와 Gate로 장기 의존성 해결
- LSTM Gates: Forget Gate (잊기), Input Gate (입력), Output Gate (출력)
- GRU (Gated Recurrent Unit): LSTM 단순화, 2개 Gate, 빠름
- Bidirectional RNN: 양방향 처리, 과거+미래 정보 활용
- Seq2Seq: Encoder-Decoder 구조, 기계 번역, 챗봇
- Attention Mechanism: 중요한 부분 집중, Transformer의 기초
- 활용: 언어 모델, 번역, 음성 인식, 주가 예측, 비디오 분석

## 최신 트렌드
- Transformer 대체
- Bidirectional RNN

# 정의
시퀀스 데이터의 시간적 의존성을 학습하는 순환 신경망으로, 자연어와 시계열 처리에 사용됩니다.

## 특징
- RNN (Recurrent Neural Network): 이전 상태를 현재 입력에 반영, 순환 구조
- 문제점: Vanishing Gradient (기울기 소실), Long-term Dependency 학습 어려움
- LSTM (Long Short-Term Memory): Cell State와 Gate로 장기 의존성 해결
- LSTM Gates: Forget Gate (잊기), Input Gate (입력), Output Gate (출력)
- GRU (Gated Recurrent Unit): LSTM 단순화, 2개 Gate, 빠름
- Bidirectional RNN: 양방향 처리, 과거+미래 정보 활용
- Seq2Seq: Encoder-Decoder 구조, 기계 번역, 챗봇
- Attention Mechanism: 중요한 부분 집중, Transformer의 기초
- 활용: 언어 모델, 번역, 음성 인식, 주가 예측, 비디오 분석

## 최신 트렌드
- Transformer 대체
- Bidirectional RNN