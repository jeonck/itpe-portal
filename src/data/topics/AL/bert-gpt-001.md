---
category: technical-focus
certifications:
- information-management
difficulty: advanced
id: bert-gpt-001
importance: 5
keywords:
- BERT
- GPT
- Masked LM
- Autoregressive
- Bidirectional
- Decoder-only
relatedTopics:
- transformer-001
- llm-001
subcategory: 알고리즘
subjectCategories:
- AL
- DS
tags:
- '2025'
title: BERT vs GPT 구조
trends:
- Instruction Tuning
- RLHF
---

# 정의
Transformer 기반의 두 대표적인 사전 학습 모델로, 양방향(BERT)과 단방향(GPT) 언어 이해 방식의 차이를 가집니다.

## 특징
- BERT (Bidirectional Encoder Representations from Transformers): Encoder-only, 양방향
- BERT 학습: Masked Language Model (MLM), 일부 단어 가림 → 예측
- Next Sentence Prediction (NSP): 두 문장 연속성 예측
- BERT 활용: 질의응답, 감정 분석, 개체명 인식 (Fine-tuning)
- GPT (Generative Pre-trained Transformer): Decoder-only, 단방향 (왼→오)
- GPT 학습: Autoregressive LM, 이전 토큰으로 다음 토큰 예측
- GPT 활용: 텍스트 생성, 대화, Zero-shot/Few-shot Learning
- 차이점: BERT는 이해(Understanding), GPT는 생성(Generation)
- GPT-3/4: 대규모 파라미터, In-Context Learning, Prompt Engineering
- 최신: Instruction Tuning, RLHF (Reinforcement Learning from Human Feedback)

## 최신 트렌드
- Instruction Tuning
- RLHF

# 정의
Transformer 기반의 두 대표적인 사전 학습 모델로, 양방향(BERT)과 단방향(GPT) 언어 이해 방식의 차이를 가집니다.

## 특징
- BERT (Bidirectional Encoder Representations from Transformers): Encoder-only, 양방향
- BERT 학습: Masked Language Model (MLM), 일부 단어 가림 → 예측
- Next Sentence Prediction (NSP): 두 문장 연속성 예측
- BERT 활용: 질의응답, 감정 분석, 개체명 인식 (Fine-tuning)
- GPT (Generative Pre-trained Transformer): Decoder-only, 단방향 (왼→오)
- GPT 학습: Autoregressive LM, 이전 토큰으로 다음 토큰 예측
- GPT 활용: 텍스트 생성, 대화, Zero-shot/Few-shot Learning
- 차이점: BERT는 이해(Understanding), GPT는 생성(Generation)
- GPT-3/4: 대규모 파라미터, In-Context Learning, Prompt Engineering
- 최신: Instruction Tuning, RLHF (Reinforcement Learning from Human Feedback)

## 최신 트렌드
- Instruction Tuning
- RLHF