---
category: digital-service
certifications:
- information-management
- computer-systems
difficulty: advanced
id: llm-001
importance: 5
keywords:
- Transformer
- GPT
- BERT
- 프롬프트 엔지니어링
- RAG
relatedTopics:
- ai-deep-learning-001
- rag-001
- prompt-engineering-001
subcategory: AI/ML
subjectCategories:
- DS
title: LLM (거대언어모델)
trends:
- RAG (검색증강생성)
- 멀티모달 LLM
- LLMOps
- 에이전트 AI
---

# 정의
수백억~수조 개의 파라미터로 구성된 거대 언어 모델로, Transformer 아키텍처 기반의 자연어 처리 AI 아키텍처.


## 특징
- Transformer 아키텍처: Self-Attention 메커니즘으로 문맥 이해, Encoder-Decoder 또는 Decoder-only 구조
- 대표 모델: GPT(생성), BERT(이해), T5(통합). GPT-4는 1.7조 파라미터 추정
- Few-shot/Zero-shot Learning: 적은 예시 또는 예시 없이도 새로운 태스크 수행
- Prompt Engineering: 입력 프롬프트 설계로 모델 출력 제어. Chain-of-Thought, In-Context Learning
- RAG(Retrieval-Augmented Generation): 외부 지식 검색 후 생성하여 환각(Hallucination) 감소

## 기술요소
거대 언어 모델(LLM)은 주로 트랜스포머(Transformer) 아키텍처를 기반으로 하며, 다음과 같은 핵심 기술 요소들을 포함합니다.

-   **트랜스포머 (Transformer) 아키텍처**:
    -   **정의**: 2017년 Google에서 제안한 시퀀스-투-시퀀스(Sequence-to-Sequence) 모델로, 순환 신경망(RNN)이나 합성곱 신경망(CNN) 없이 어텐션 메커니즘만으로 구성됩니다.
    -   **장점**: 병렬 처리 능력이 뛰어나 학습 속도가 빠르며, 장거리 의존성 문제를 효과적으로 해결합니다.
    -   **주요 구성**:
        -   **인코더 (Encoder)**: 입력 시퀀스(`x`)를 받아 문맥 정보를 추출하고 벡터 표현으로 변환합니다. 여러 개의 인코더 블록으로 구성.
        -   **디코더 (Decoder)**: 인코더의 출력과 이전 단계의 출력(`y`)을 받아 다음 단어를 생성합니다. 여러 개의 디코더 블록으로 구성.
        -   **어텐션 메커니즘 (Attention Mechanism)**: 입력 시퀀스의 모든 부분에 동시에 주의를 기울여, 각 출력 단어를 생성할 때 입력 시퀀스에서 가장 관련 있는 부분에 가중치를 부여합니다.
            -   **Self-Attention**: 단어 자체가 다른 단어들과 어떤 관계를 가지는지 학습.
            -   **Multi-Head Attention**: 여러 개의 어텐션 헤드를 병렬로 사용하여 다양한 관점에서 정보 통합.
        -   **위치 임베딩 (Positional Encoding)**: 단어의 순서 정보가 없는 트랜스포머에 단어의 위치 정보를 제공합니다.

-   **거대 언어 모델 유형**:
    -   **GPT (Generative Pre-trained Transformer)**:
        -   **구조**: 디코더(Decoder-only) 스택으로 구성.
        -   **특징**: 비지도 학습으로 대량의 텍스트 데이터를 사전 학습한 후, 파인튜닝을 통해 다양한 생성(Generation) 태스크를 수행합니다.
        -   **활용**: 텍스트 생성, 번역, 요약, 질의응답 등.
    -   **BERT (Bidirectional Encoder Representations from Transformers)**:
        -   **구조**: 인코더(Encoder-only) 스택으로 구성.
        -   **특징**: 양방향 문맥(Bidirectional Context)을 사전 학습하여 자연어 이해(NLU) 태스크에 강점.
        -   **활용**: 감성 분석, 개체명 인식, 문장 분류 등.

-   **프롬프트 엔지니어링 (Prompt Engineering)**:
    -   **정의**: LLM으로부터 원하는 응답을 얻기 위해 입력 프롬프트(지시문)를 설계하고 최적화하는 기술.
    -   **기법**:
        -   **Few-shot / Zero-shot Learning**: 모델에게 몇 가지 예시를 주거나(few-shot) 아예 주지 않고도(zero-shot) 새로운 태스크를 수행하도록 유도.
        -   **Chain-of-Thought (CoT) Prompting**: 복잡한 문제를 단계별로 생각하도록 유도하여 추론 능력을 향상.
        -   **In-Context Learning**: 프롬프트 내에 예시를 제공하여 모델이 태스크를 이해하도록 돕습니다.

-   **RAG (Retrieval-Augmented Generation, 검색 증강 생성)**:
    -   **정의**: LLM이 답변을 생성하기 전에 외부 데이터베이스나 문서에서 관련 정보를 검색(Retrieval)하고, 이를 바탕으로 답변을 생성(Generation)하는 기술.
    -   **장점**: 모델의 환각(Hallucination) 현상을 줄이고, 최신 정보 및 도메인 특정 지식을 활용하여 답변의 정확성과 신뢰도를 높입니다.

이러한 기술 요소들은 LLM이 인간의 언어를 이해하고 생성하는 능력을 고도화하여 다양한 AI 서비스의 핵심 엔진으로 활용될 수 있도록 합니다.