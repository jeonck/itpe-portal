---
category: digital-service
certifications:
- information-management
- computer-systems
difficulty: advanced
id: ai-security-001
importance: 5
keywords:
- AI 보안
- 적대적 공격
- 데이터 오염
- 모델 탈취
- 프라이버시 침해
relatedTopics:
- ai-deep-learning-001
- encryption-001
subcategory: AI 보안
subjectCategories:
- IS
- DS
title: AI 보안 (Adversarial Attack, Poisoning)
trends:
- AI 모델 방어 기술
- Federated Learning 보안
- XAI와 보안
---

# 정의
AI 보안은 인공지능 시스템 자체를 대상으로 하는 다양한 공격(적대적 공격, 데이터 오염 등)으로부터 AI 모델과 데이터를 보호하고, AI 시스템의 안전성과 신뢰성을 확보하는 분야입니다. AI 기술의 확산과 함께 그 중요성이 증대되고 있으며, '방어'를 넘어 'AI 악용 방지'가 핵심 과제로 부상하고 있습니다 기술.

## 특징
- 적대적 공격 (Adversarial Attack): AI 모델이 잘못된 예측을 하도록 유도하기 위해 미세하게 조작된 입력 데이터를 생성하는 기법. 육안으로는 구별하기 어렵지만, AI 모델을 오작동시킵니다.
  - 회피 공격 (Evasion Attack): 모델의 예측을 회피하도록 입력 데이터를 조작. (예: 스팸 필터를 우회하는 스팸 메일)
  - 표적 공격 (Targeted Attack): 특정 클래스로 오분류되도록 입력 데이터를 조작. (예: 정지 표지판을 속도 제한 표지판으로 오인식)
- 데이터 오염 공격 (Data Poisoning Attack): AI 모델 학습 데이터에 악의적인 데이터를 주입하여 모델의 정확도를 저하시키거나 특정 의도를 가진 예측을 하도록 조작하는 기법.
- 모델 탈취 공격 (Model Extraction/Stealing): AI 모델의 파라미터나 구조를 복제하거나 재구성하여 유사한 성능을 내는 모델을 만들어내는 공격.
- 프라이버시 침해 공격: AI 모델이 학습 데이터를 유출하거나 재구성하여 개인 정보를 알아내는 공격. (예: 멤버십 추론 공격)
- 방어 기술: 적대적 학습(Adversarial Training), 입력 정화(Input Sanitization), 모델 앙상블(Model Ensembling), Secure Multi-Party Computation (SMC), Federated Learning 등.

## 기술요소
AI 보안을 위한 주요 기술 요소들은 AI 시스템을 대상으로 하는 공격 유형과 이를 방어하는 대책으로 구성됩니다.

-   AI 공격 유형:
    -   적대적 공격 (Adversarial Attack):
        -   원리: AI 모델이 잘못된 예측을 하도록 유도하기 위해 미세하게 조작된 입력 데이터를 생성하는 기법. 육안으로는 구별하기 어렵지만, AI 모델을 오작동시킵니다.
        -   회피 공격 (Evasion Attack): 모델의 예측을 회피하도록 입력 데이터를 조작. (예: 악성코드 탐지 모델을 우회하는 악성코드)
        -   표적 공격 (Targeted Attack): 특정 클래스로 오분류되도록 입력 데이터를 조작. (예: 정지 표지판을 속도 제한 표지판으로 오인식)
        -   원인: AI 모델의 결정 경계(Decision Boundary)가 인간과 다르게 동작하는 지점을 이용합니다.
    -   데이터 오염 공격 (Data Poisoning Attack):
        -   원리: AI 모델 학습 데이터에 악의적인 데이터를 주입하여 모델의 정확도를 저하시키거나 특정 의도를 가진 예측을 하도록 조작하는 기법. 학습 단계에서 모델을 오염시킵니다.
        -   목표: 모델의 성능 저하, 백도어 심기, 특정 입력에 대한 오분류 유도.
    -   모델 탈취 공격 (Model Extraction/Stealing):
        -   원리: AI 모델의 파라미터나 구조를 복제하거나 재구성하여 유사한 성능을 내는 모델을 만들어내는 공격. 모델의 API에 질의를 반복하여 모델의 동작을 유추.
        -   목표: 지적 재산권 침해, 모델을 이용한 다른 공격 수행.
    -   프라이버시 침해 공격:
        -   원리: AI 모델이 학습 데이터를 유출하거나 재구성하여 개인 정보를 알아내는 공격.
        -   멤버십 추론 공격 (Membership Inference Attack): 특정 데이터가 모델 학습에 사용되었는지 여부를 추론.
        -   모델 역공학 (Model Inversion): 모델 출력으로부터 학습 데이터의 특징을 재구성.

-   AI 방어 기술:
    -   적대적 학습 (Adversarial Training): 적대적 샘플을 학습 데이터에 포함하여 모델의 견고성을 높이는 방어 기법.
    -   입력 정화 (Input Sanitization): AI 모델에 입력되기 전에 데이터를 필터링하거나 정규화하여 악의적인 조작을 제거.
    -   모델 앙상블 (Model Ensembling): 여러 AI 모델을 조합하여 사용함으로써 단일 모델의 취약점을 보완하고 공격에 대한 강건성을 높입니다.
    -   Secure Multi-Party Computation (SMC): 여러 주체가 각자의 데이터를 공개하지 않고 연합하여 공동으로 AI 모델을 학습하거나 추론할 수 있도록 하는 암호화 기술.
    -   Federated Learning (연합 학습): 분산된 디바이스나 서버가 각자의 로컬 데이터를 사용하여 모델을 학습하고, 학습된 모델의 파라미터만 중앙 서버로 전송하여 통합하는 방식. 데이터 자체는 이동하지 않아 프라이버시 보호에 유리합니다.
    -   Differential Privacy (차분 프라이버시): 학습 데이터에 의도적인 노이즈를 주입하여 개별 데이터의 정보 유출 위험을 수학적으로 보장하며 최소화.

이러한 공격과 방어 기술에 대한 이해는 안전하고 신뢰할 수 있는 AI 시스템을 구축하는 데 필수적입니다.

# 정의
AI 보안은 인공지능 시스템 자체를 대상으로 하는 다양한 공격(적대적 공격, 데이터 오염 등)으로부터 AI 모델과 데이터를 보호하고, AI 시스템의 안전성과 신뢰성을 확보하는 분야입니다. AI 기술의 확산과 함께 그 중요성이 증대되고 있으며, '방어'를 넘어 'AI 악용 방지'가 핵심 과제로 부상하고 있습니다.

## 특징
- 적대적 공격 (Adversarial Attack): AI 모델이 잘못된 예측을 하도록 유도하기 위해 미세하게 조작된 입력 데이터를 생성하는 기법. 육안으로는 구별하기 어렵지만, AI 모델을 오작동시킵니다.
  - 회피 공격 (Evasion Attack): 모델의 예측을 회피하도록 입력 데이터를 조작. (예: 스팸 필터를 우회하는 스팸 메일)
  - 표적 공격 (Targeted Attack): 특정 클래스로 오분류되도록 입력 데이터를 조작. (예: 정지 표지판을 속도 제한 표지판으로 오인식)
- 데이터 오염 공격 (Data Poisoning Attack): AI 모델 학습 데이터에 악의적인 데이터를 주입하여 모델의 정확도를 저하시키거나 특정 의도를 가진 예측을 하도록 조작하는 기법.
- 모델 탈취 공격 (Model Extraction/Stealing): AI 모델의 파라미터나 구조를 복제하거나 재구성하여 유사한 성능을 내는 모델을 만들어내는 공격.
- 프라이버시 침해 공격: AI 모델이 학습 데이터를 유출하거나 재구성하여 개인 정보를 알아내는 공격. (예: 멤버십 추론 공격)
- 방어 기술: 적대적 학습(Adversarial Training), 입력 정화(Input Sanitization), 모델 앙상블(Model Ensembling), Secure Multi-Party Computation (SMC), Federated Learning 등.

## 기술요소
AI 보안을 위한 주요 기술 요소들은 AI 시스템을 대상으로 하는 공격 유형과 이를 방어하는 대책으로 구성됩니다.

-   AI 공격 유형:
    -   적대적 공격 (Adversarial Attack):
        -   원리: AI 모델이 잘못된 예측을 하도록 유도하기 위해 미세하게 조작된 입력 데이터를 생성하는 기법. 육안으로는 구별하기 어렵지만, AI 모델을 오작동시킵니다.
        -   회피 공격 (Evasion Attack): 모델의 예측을 회피하도록 입력 데이터를 조작. (예: 악성코드 탐지 모델을 우회하는 악성코드)
        -   표적 공격 (Targeted Attack): 특정 클래스로 오분류되도록 입력 데이터를 조작. (예: 정지 표지판을 속도 제한 표지판으로 오인식)
        -   원인: AI 모델의 결정 경계(Decision Boundary)가 인간과 다르게 동작하는 지점을 이용합니다.
    -   데이터 오염 공격 (Data Poisoning Attack):
        -   원리: AI 모델 학습 데이터에 악의적인 데이터를 주입하여 모델의 정확도를 저하시키거나 특정 의도를 가진 예측을 하도록 조작하는 기법. 학습 단계에서 모델을 오염시킵니다.
        -   목표: 모델의 성능 저하, 백도어 심기, 특정 입력에 대한 오분류 유도.
    -   모델 탈취 공격 (Model Extraction/Stealing):
        -   원리: AI 모델의 파라미터나 구조를 복제하거나 재구성하여 유사한 성능을 내는 모델을 만들어내는 공격. 모델의 API에 질의를 반복하여 모델의 동작을 유추.
        -   목표: 지적 재산권 침해, 모델을 이용한 다른 공격 수행.
    -   프라이버시 침해 공격:
        -   원리: AI 모델이 학습 데이터를 유출하거나 재구성하여 개인 정보를 알아내는 공격.
        -   멤버십 추론 공격 (Membership Inference Attack): 특정 데이터가 모델 학습에 사용되었는지 여부를 추론.
        -   모델 역공학 (Model Inversion): 모델 출력으로부터 학습 데이터의 특징을 재구성.

-   AI 방어 기술:
    -   적대적 학습 (Adversarial Training): 적대적 샘플을 학습 데이터에 포함하여 모델의 견고성을 높이는 방어 기법.
    -   입력 정화 (Input Sanitization): AI 모델에 입력되기 전에 데이터를 필터링하거나 정규화하여 악의적인 조작을 제거.
    -   모델 앙상블 (Model Ensembling): 여러 AI 모델을 조합하여 사용함으로써 단일 모델의 취약점을 보완하고 공격에 대한 강건성을 높입니다.
    -   Secure Multi-Party Computation (SMC): 여러 주체가 각자의 데이터를 공개하지 않고 연합하여 공동으로 AI 모델을 학습하거나 추론할 수 있도록 하는 암호화 기술.
    -   Federated Learning (연합 학습): 분산된 디바이스나 서버가 각자의 로컬 데이터를 사용하여 모델을 학습하고, 학습된 모델의 파라미터만 중앙 서버로 전송하여 통합하는 방식. 데이터 자체는 이동하지 않아 프라이버시 보호에 유리합니다.
    -   Differential Privacy (차분 프라이버시): 학습 데이터에 의도적인 노이즈를 주입하여 개별 데이터의 정보 유출 위험을 수학적으로 보장하며 최소화.

이러한 공격과 방어 기술에 대한 이해는 안전하고 신뢰할 수 있는 AI 시스템을 구축하는 데 필수적입니다.